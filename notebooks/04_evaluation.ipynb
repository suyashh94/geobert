{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-header",
   "metadata": {},
   "source": [
    "# GeoBERT Model Evaluation\n",
    "\n",
    "This notebook evaluates the trained GeoBERT model on the test set and visualizes predictions using interactive Folium maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "from geobert import Inferencer\n",
    "from geobert.metrics import haversine_distance_m\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CHECKPOINT_DIR = Path(\"../outputs/checkpoints\")\n",
    "TEST_DATA_PATH = CHECKPOINT_DIR / \"test_data.parquet\"\n",
    "\n",
    "# NYC center for maps\n",
    "NYC_CENTER = [40.7128, -74.0060]\n",
    "\n",
    "# Error thresholds for color coding (meters)\n",
    "ERROR_THRESHOLDS = {\n",
    "    'good': 100,      # < 100m = green\n",
    "    'medium': 500,    # 100-500m = yellow\n",
    "    # > 500m = red\n",
    "}\n",
    "\n",
    "ERROR_COLORS = {\n",
    "    'good': 'green',\n",
    "    'medium': 'orange',\n",
    "    'bad': 'red'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-load-header",
   "metadata": {},
   "source": [
    "## 1. Load Model and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inferencer\n",
    "print(f\"Loading model from: {CHECKPOINT_DIR}\")\n",
    "inferencer = Inferencer(CHECKPOINT_DIR)\n",
    "print(f\"Model loaded on device: {inferencer.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(f\"Loading test data from: {TEST_DATA_PATH}\")\n",
    "test_df = pd.read_parquet(TEST_DATA_PATH)\n",
    "print(f\"Test set size: {len(test_df):,} samples\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-inference-header",
   "metadata": {},
   "source": [
    "## 2. Run Inference on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-inference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch inference\n",
    "addresses = test_df['address'].tolist()\n",
    "\n",
    "print(f\"Running inference on {len(addresses):,} addresses...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pred_lats, pred_lons = inferencer.predict_batch(addresses, batch_size=128)\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "print(f\"Inference completed in {inference_time:.2f}s\")\n",
    "print(f\"Throughput: {len(addresses) / inference_time:.0f} addresses/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-add-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to dataframe\n",
    "test_df['pred_latitude'] = pred_lats\n",
    "test_df['pred_longitude'] = pred_lons\n",
    "\n",
    "# Calculate error distances using haversine formula\n",
    "actual_lats = torch.tensor(test_df['latitude'].values)\n",
    "actual_lons = torch.tensor(test_df['longitude'].values)\n",
    "pred_lat_tensor = torch.tensor(pred_lats)\n",
    "pred_lon_tensor = torch.tensor(pred_lons)\n",
    "\n",
    "distances_m = haversine_distance_m(actual_lats, actual_lons, pred_lat_tensor, pred_lon_tensor)\n",
    "test_df['error_m'] = distances_m.numpy()\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-metrics-header",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "lat_errors = test_df['pred_latitude'] - test_df['latitude']\n",
    "lon_errors = test_df['pred_longitude'] - test_df['longitude']\n",
    "\n",
    "metrics = {\n",
    "    'MAE Latitude (degrees)': np.abs(lat_errors).mean(),\n",
    "    'MAE Longitude (degrees)': np.abs(lon_errors).mean(),\n",
    "    'RMSE Latitude (degrees)': np.sqrt((lat_errors ** 2).mean()),\n",
    "    'RMSE Longitude (degrees)': np.sqrt((lon_errors ** 2).mean()),\n",
    "    'Mean Distance Error (m)': test_df['error_m'].mean(),\n",
    "    'Median Distance Error (m)': test_df['error_m'].median(),\n",
    "    'Std Distance Error (m)': test_df['error_m'].std(),\n",
    "    '90th Percentile Error (m)': np.percentile(test_df['error_m'], 90),\n",
    "    '95th Percentile Error (m)': np.percentile(test_df['error_m'], 95),\n",
    "    '99th Percentile Error (m)': np.percentile(test_df['error_m'], 99),\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 50)\n",
    "for name, value in metrics.items():\n",
    "    print(f\"{name:35s}: {value:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-accuracy-buckets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy at different thresholds\n",
    "thresholds = [50, 100, 200, 500, 1000, 2000]\n",
    "\n",
    "print(\"\\nAccuracy at Distance Thresholds:\")\n",
    "print(\"-\" * 40)\n",
    "for thresh in thresholds:\n",
    "    accuracy = (test_df['error_m'] < thresh).mean() * 100\n",
    "    print(f\"Within {thresh:5d}m: {accuracy:6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-distribution-header",
   "metadata": {},
   "source": [
    "## 4. Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-histogram",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of errors (capped at 2000m for visibility)\n",
    "errors_capped = test_df['error_m'].clip(upper=2000)\n",
    "axes[0].hist(errors_capped, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(test_df['error_m'].median(), color='red', linestyle='--', label=f'Median: {test_df[\"error_m\"].median():.0f}m')\n",
    "axes[0].axvline(test_df['error_m'].mean(), color='orange', linestyle='--', label=f'Mean: {test_df[\"error_m\"].mean():.0f}m')\n",
    "axes[0].set_xlabel('Error Distance (m)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Prediction Errors')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-scale histogram for full range\n",
    "axes[1].hist(test_df['error_m'], bins=50, edgecolor='black', alpha=0.7, log=True)\n",
    "axes[1].set_xlabel('Error Distance (m)')\n",
    "axes[1].set_ylabel('Count (log scale)')\n",
    "axes[1].set_title('Distribution of Prediction Errors (Log Scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-error-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize errors\n",
    "def categorize_error(error_m):\n",
    "    if error_m < ERROR_THRESHOLDS['good']:\n",
    "        return 'good'\n",
    "    elif error_m < ERROR_THRESHOLDS['medium']:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'bad'\n",
    "\n",
    "test_df['error_category'] = test_df['error_m'].apply(categorize_error)\n",
    "\n",
    "category_counts = test_df['error_category'].value_counts()\n",
    "category_pcts = category_counts / len(test_df) * 100\n",
    "\n",
    "print(\"Error Categories:\")\n",
    "print(f\"  Good (<{ERROR_THRESHOLDS['good']}m):     {category_counts.get('good', 0):,} ({category_pcts.get('good', 0):.1f}%)\")\n",
    "print(f\"  Medium ({ERROR_THRESHOLDS['good']}-{ERROR_THRESHOLDS['medium']}m): {category_counts.get('medium', 0):,} ({category_pcts.get('medium', 0):.1f}%)\")\n",
    "print(f\"  Bad (>{ERROR_THRESHOLDS['medium']}m):    {category_counts.get('bad', 0):,} ({category_pcts.get('bad', 0):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz-header",
   "metadata": {},
   "source": [
    "## 5. Interactive Map Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz1-header",
   "metadata": {},
   "source": [
    "### 5.1 Prediction vs Ground Truth\n",
    "\n",
    "Shows predicted (red) and actual (green) locations connected by lines. Click markers for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pred-vs-actual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 500 points for visualization\n",
    "sample_df = test_df.sample(n=min(500, len(test_df)), random_state=42)\n",
    "\n",
    "m_compare = folium.Map(location=NYC_CENTER, zoom_start=11, tiles='CartoDB positron')\n",
    "\n",
    "for _, row in sample_df.iterrows():\n",
    "    actual = [row['latitude'], row['longitude']]\n",
    "    predicted = [row['pred_latitude'], row['pred_longitude']]\n",
    "    \n",
    "    # Line connecting actual to predicted\n",
    "    folium.PolyLine(\n",
    "        [actual, predicted],\n",
    "        color='gray',\n",
    "        weight=1,\n",
    "        opacity=0.5\n",
    "    ).add_to(m_compare)\n",
    "    \n",
    "    # Actual location (green)\n",
    "    folium.CircleMarker(\n",
    "        location=actual,\n",
    "        radius=4,\n",
    "        color='green',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"<b>Actual</b><br>{row['address']}<br>Error: {row['error_m']:.0f}m\"\n",
    "    ).add_to(m_compare)\n",
    "    \n",
    "    # Predicted location (red)\n",
    "    folium.CircleMarker(\n",
    "        location=predicted,\n",
    "        radius=3,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"<b>Predicted</b><br>{row['address']}<br>Error: {row['error_m']:.0f}m\"\n",
    "    ).add_to(m_compare)\n",
    "\n",
    "# Add legend\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; background-color: white;\n",
    "            padding: 10px; border: 2px solid gray; border-radius: 5px;\">\n",
    "    <b>Legend</b><br>\n",
    "    <i style=\"background: green; width: 12px; height: 12px; display: inline-block; border-radius: 50%;\"></i> Actual<br>\n",
    "    <i style=\"background: red; width: 12px; height: 12px; display: inline-block; border-radius: 50%;\"></i> Predicted<br>\n",
    "    <i style=\"background: gray; width: 20px; height: 2px; display: inline-block;\"></i> Error line\n",
    "</div>\n",
    "'''\n",
    "m_compare.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "m_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz2-header",
   "metadata": {},
   "source": [
    "### 5.2 Error Heatmap\n",
    "\n",
    "Geographic distribution of prediction errors. Brighter areas indicate higher error concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-error-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap weighted by error magnitude\n",
    "# Use a larger sample for better heatmap coverage\n",
    "heat_sample = test_df.sample(n=min(10000, len(test_df)), random_state=42)\n",
    "\n",
    "# Normalize errors for heatmap weights (0-1 scale, capped at 1000m)\n",
    "max_error_for_weight = 1000\n",
    "heat_sample['weight'] = (heat_sample['error_m'].clip(upper=max_error_for_weight) / max_error_for_weight)\n",
    "\n",
    "# Create weighted heatmap data: [lat, lon, weight]\n",
    "heat_data = heat_sample[['latitude', 'longitude', 'weight']].values.tolist()\n",
    "\n",
    "m_heat = folium.Map(location=NYC_CENTER, zoom_start=11, tiles='CartoDB dark_matter')\n",
    "\n",
    "HeatMap(\n",
    "    heat_data,\n",
    "    min_opacity=0.3,\n",
    "    radius=12,\n",
    "    blur=15,\n",
    "    max_zoom=13\n",
    ").add_to(m_heat)\n",
    "\n",
    "# Add title\n",
    "title_html = '''\n",
    "<div style=\"position: fixed; top: 10px; left: 50%; transform: translateX(-50%); z-index: 1000; \n",
    "            background-color: rgba(0,0,0,0.7); color: white; padding: 10px; border-radius: 5px;\">\n",
    "    <b>Prediction Error Heatmap</b><br>\n",
    "    <small>Brighter = Higher Errors</small>\n",
    "</div>\n",
    "'''\n",
    "m_heat.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "m_heat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-viz3-header",
   "metadata": {},
   "source": [
    "### 5.3 Error-Colored Markers\n",
    "\n",
    "Markers colored by error magnitude:\n",
    "- **Green**: < 100m (good)\n",
    "- **Orange**: 100-500m (medium)\n",
    "- **Red**: > 500m (bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-error-markers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for error-colored visualization\n",
    "marker_sample = test_df.sample(n=min(1000, len(test_df)), random_state=42)\n",
    "\n",
    "m_errors = folium.Map(location=NYC_CENTER, zoom_start=11, tiles='CartoDB positron')\n",
    "\n",
    "for _, row in marker_sample.iterrows():\n",
    "    color = ERROR_COLORS[row['error_category']]\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=4,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"{row['address']}<br>Error: {row['error_m']:.0f}m\"\n",
    "    ).add_to(m_errors)\n",
    "\n",
    "# Add legend\n",
    "legend_html = f'''\n",
    "<div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; background-color: white;\n",
    "            padding: 10px; border: 2px solid gray; border-radius: 5px;\">\n",
    "    <b>Error Categories</b><br>\n",
    "    <i style=\"background: green; width: 12px; height: 12px; display: inline-block; border-radius: 50%;\"></i> Good (&lt;{ERROR_THRESHOLDS['good']}m)<br>\n",
    "    <i style=\"background: orange; width: 12px; height: 12px; display: inline-block; border-radius: 50%;\"></i> Medium ({ERROR_THRESHOLDS['good']}-{ERROR_THRESHOLDS['medium']}m)<br>\n",
    "    <i style=\"background: red; width: 12px; height: 12px; display: inline-block; border-radius: 50%;\"></i> Bad (&gt;{ERROR_THRESHOLDS['medium']}m)\n",
    "</div>\n",
    "'''\n",
    "m_errors.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "m_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-worst-header",
   "metadata": {},
   "source": [
    "### 5.4 Worst Predictions\n",
    "\n",
    "Visualize the predictions with the highest errors to understand failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-worst-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 50 worst predictions\n",
    "worst_df = test_df.nlargest(50, 'error_m')\n",
    "\n",
    "print(f\"Top 10 Worst Predictions:\")\n",
    "print(worst_df[['address', 'error_m']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-worst-map",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_worst = folium.Map(location=NYC_CENTER, zoom_start=10, tiles='CartoDB positron')\n",
    "\n",
    "for _, row in worst_df.iterrows():\n",
    "    actual = [row['latitude'], row['longitude']]\n",
    "    predicted = [row['pred_latitude'], row['pred_longitude']]\n",
    "    \n",
    "    # Line connecting actual to predicted\n",
    "    folium.PolyLine(\n",
    "        [actual, predicted],\n",
    "        color='red',\n",
    "        weight=2,\n",
    "        opacity=0.7\n",
    "    ).add_to(m_worst)\n",
    "    \n",
    "    # Actual location\n",
    "    folium.CircleMarker(\n",
    "        location=actual,\n",
    "        radius=6,\n",
    "        color='green',\n",
    "        fill=True,\n",
    "        popup=f\"<b>Actual</b><br>{row['address']}<br>Error: {row['error_m']:.0f}m\"\n",
    "    ).add_to(m_worst)\n",
    "    \n",
    "    # Predicted location\n",
    "    folium.CircleMarker(\n",
    "        location=predicted,\n",
    "        radius=5,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        popup=f\"<b>Predicted</b><br>Error: {row['error_m']:.0f}m\"\n",
    "    ).add_to(m_worst)\n",
    "\n",
    "m_worst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-save-header",
   "metadata": {},
   "source": [
    "## 6. Save Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-save-maps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"../outputs/maps\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "maps_to_save = {\n",
    "    'evaluation_pred_vs_actual.html': m_compare,\n",
    "    'evaluation_error_heatmap.html': m_heat,\n",
    "    'evaluation_error_markers.html': m_errors,\n",
    "    'evaluation_worst_predictions.html': m_worst,\n",
    "}\n",
    "\n",
    "for filename, map_obj in maps_to_save.items():\n",
    "    filepath = output_dir / filename\n",
    "    map_obj.save(str(filepath))\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary-header",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTest Set Size: {len(test_df):,} samples\")\n",
    "print(f\"Inference Time: {inference_time:.2f}s ({len(test_df) / inference_time:.0f} samples/sec)\")\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  Mean Distance Error:   {metrics['Mean Distance Error (m)']:.1f}m\")\n",
    "print(f\"  Median Distance Error: {metrics['Median Distance Error (m)']:.1f}m\")\n",
    "print(f\"  90th Percentile Error: {metrics['90th Percentile Error (m)']:.1f}m\")\n",
    "print(f\"\\nError Distribution:\")\n",
    "print(f\"  Good (<{ERROR_THRESHOLDS['good']}m):     {category_pcts.get('good', 0):.1f}%\")\n",
    "print(f\"  Medium ({ERROR_THRESHOLDS['good']}-{ERROR_THRESHOLDS['medium']}m): {category_pcts.get('medium', 0):.1f}%\")\n",
    "print(f\"  Bad (>{ERROR_THRESHOLDS['medium']}m):    {category_pcts.get('bad', 0):.1f}%\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
